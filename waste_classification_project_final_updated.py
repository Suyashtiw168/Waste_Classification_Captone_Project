# -*- coding: utf-8 -*-
"""Waste_Classification_Project_Final_Updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PqiH7cJwdiHaB8lTNcsH_xRXThn1jrti
"""

!ls /content

from google.colab import drive
drive.mount('/content/drive')



from google.colab import drive
drive.mount('/content/drive')

# Copy dataset from Drive to Colab local (fast)
!cp -r "/content/drive/MyDrive/Garbage_Dataset_Classification/images" "/content/images"

!ls /content/images



# ================================
# Fast Transfer Learning - ResNet50
# ================================

# 1. Imports
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing import image_dataset_from_directory
import matplotlib.pyplot as plt

# 2. Dataset Path
data_dir = "/content/images"   # <-- yahi mount kiya tha

# 3. Parameters
image_size = (128, 128)
batch_size = 32   # thoda chhota batch -> GPU fast chalega

# 4. Train/Validation Split
train_ds = image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=image_size,
    batch_size=batch_size
)
val_ds = image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=image_size,
    batch_size=batch_size
)

class_names = train_ds.class_names
print("âœ… Classes:", class_names)

# 5. Prefetch
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)

# 6. Model - ResNet50 base (frozen)
base_model = ResNet50(weights="imagenet", include_top=False, input_shape=(128,128,3))
for layer in base_model.layers:
    layer.trainable = False   # freeze for fast training

# 7. Custom head
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(64, activation="relu")(x)
x = layers.Dropout(0.3)(x)
output = layers.Dense(len(class_names), activation="softmax")(x)

model = models.Model(inputs=base_model.input, outputs=output)

# 8. Compile
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

# 9. Train (FAST: only 2 epochs)
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=2
)

# 10. Save model
model.save("/content/drive/MyDrive/waste_resnet50_fast2.keras")
print("âœ… Model saved!")

# 11. Plot Curves
plt.figure(figsize=(8,4))
plt.plot(history.history["accuracy"], label="Train Acc")
plt.plot(history.history["val_accuracy"], label="Val Acc")
plt.legend()
plt.title("Accuracy Curve")
plt.show()

plt.figure(figsize=(8,4))
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.legend()
plt.title("Loss Curve")
plt.show()

import numpy as np
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import tensorflow as tf

# ðŸ”¹ Load trained model
model = tf.keras.models.load_model("/content/drive/MyDrive/waste_resnet50_fast2.keras")

# ðŸ”¹ Class names (same as training)
class_names = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']

# ðŸ”¹ Function to test one image
def test_single_image(img_path):
    img = image.load_img(img_path, target_size=(128,128))
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)   # normalize

    preds = model.predict(img_array)
    predicted_class = class_names[np.argmax(preds)]

    # Show result
    plt.imshow(img)
    plt.axis("off")
    plt.title(f"Predicted: {predicted_class}")
    plt.show()

    print("âœ… Predicted Class:", predicted_class)

# ðŸ”¹ Example test
test_single_image("/content/images/plastic/plastic_00003.jpg")

# âœ… Random test with True + Predicted labels
def test_random_images_with_labels(base_dir="/content/images", n=10):
    plt.figure(figsize=(15, 10))

    all_paths = []
    for cls in class_names:
        class_dir = os.path.join(base_dir, cls)
        files = os.listdir(class_dir)
        files = [os.path.join(class_dir, f) for f in files]
        all_paths.extend([(f, cls) for f in files])  # (path, true_class)

    sample_paths = random.sample(all_paths, n)

    for i, (img_path, true_cls) in enumerate(sample_paths):
        img = image.load_img(img_path, target_size=(128,128))
        img_array = image.img_to_array(img)
        img_array = np.expand_dims(img_array, axis=0)   # no /255.0

        preds = model.predict(img_array, verbose=0)
        pred_class = class_names[np.argmax(preds)]

        plt.subplot(2, 5, i+1)
        plt.imshow(img)
        plt.axis("off")
        plt.title(f"True: {true_cls}\nPred: {pred_class}", fontsize=9)

    plt.show()

# Run
test_random_images_with_labels("/content/images", n=10)

model.save("waste_model.keras")

